# Tuning the KL penalty


Again, this makes sense: When the penalty coefficient `beta` is small,
gradient descent doesn't optimize for the KL penalty.

last generation for each

optimal value of `beta`
is
somewhere between
